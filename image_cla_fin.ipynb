{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe4JjsZQcS_m"
      },
      "source": [
        "##Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8MBpkCb3jhe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import imghdr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3, EfficientNetB7, ResNet101, VGG16, VGG19\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tf2onnx import convert\n",
        "import pandas as pd\n",
        "from tensorflow.keras import regularizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-U9usZN0ooJ"
      },
      "source": [
        "##Image Augmentation Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVgokSbpcEvJ"
      },
      "outputs": [],
      "source": [
        "class ImageAugmentor:\n",
        "    def __init__(self, img_size=(475, 550), batch_size=32, oversample_factor=2, num_additional_images=300, output_path=None):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.oversample_factor = oversample_factor\n",
        "        self.num_additional_images = num_additional_images\n",
        "        self.output_path = output_path\n",
        "        self.datagen = ImageDataGenerator(\n",
        "            zoom_range=0.15,\n",
        "            horizontal_flip=True,\n",
        "            rotation_range=25,\n",
        "            shear_range=0.1,\n",
        "            featurewise_center=False,\n",
        "            featurewise_std_normalization=False,\n",
        "        )\n",
        "\n",
        "    def load_minority_class_images(self, minority_class_path):\n",
        "        minority_class_images = [os.path.join(minority_class_path, img) for img in os.listdir(minority_class_path)]\n",
        "        return minority_class_images\n",
        "\n",
        "    def augment_images(self, minority_class_images):\n",
        "        augmented_images = []\n",
        "        for img_path in minority_class_images:\n",
        "            try:\n",
        "                img = cv2.imread(img_path)\n",
        "                img_resized = cv2.resize(img, self.img_size, interpolation=cv2.INTER_AREA)\n",
        "                x = image.img_to_array(img_resized)\n",
        "                x = np.expand_dims(x, axis=0)\n",
        "                i = 0\n",
        "                for batch in self.datagen.flow(x, batch_size=1):\n",
        "                    augmented_images.append(batch[0])\n",
        "                    i += 1\n",
        "                    if i >= self.oversample_factor:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image {img_path}: {str(e)}\")\n",
        "        return augmented_images\n",
        "\n",
        "    def save_augmented_images(self, augmented_images):\n",
        "        if not os.path.exists(self.output_path):\n",
        "            os.makedirs(self.output_path)\n",
        "\n",
        "        for i, augmented_img in enumerate(augmented_images[:self.num_additional_images]):\n",
        "            img_filename = f\"augmented_image_{i + 1}.jpg\"\n",
        "            img_path = os.path.join(self.output_path, img_filename)\n",
        "            augmented_img_rgb = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB)\n",
        "            cv2.imwrite(img_path, augmented_img_rgb)\n",
        "\n",
        "        print(f\"{self.num_additional_images} augmented images saved to {self.output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U__J7vXl4Ea8",
        "outputId": "81a344c7-fdee-4028-f1c7-0fb9ed7039e2"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    OUTPUT_PATH = r'C:\\Users\\ONE BY ONE\\Desktop\\Image classification\\images- 9 sites\\petra-Treasury'\n",
        "    MINORITY_CLASS_PATH = r'C:\\Users\\ONE BY ONE\\Desktop\\Image classification\\images- 9 sites\\petra-Treasury'\n",
        "    augmentor = ImageAugmentor(\n",
        "        img_size=(475, 500),\n",
        "        oversample_factor=1,\n",
        "        num_additional_images=300,\n",
        "        output_path=OUTPUT_PATH\n",
        "    )\n",
        "    augmentor = ImageAugmentor(output_path=OUTPUT_PATH)\n",
        "    minority_class_images = augmentor.load_minority_class_images(MINORITY_CLASS_PATH)\n",
        "    augmented_images = augmentor.augment_images(minority_class_images)\n",
        "    augmentor.save_augmented_images(augmented_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKAHIhY2cgnA"
      },
      "source": [
        "##Image Format Checking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmvM-hC3cEvN"
      },
      "outputs": [],
      "source": [
        "\n",
        "counter=[]\n",
        "\n",
        "image_directory = r'C:\\Users\\ONE BY ONE\\Desktop\\images- 9 sites\\petra-Treasury'\n",
        "image_files = os.listdir(image_directory)\n",
        "\n",
        "for image_file in image_files:\n",
        "    try:\n",
        "        image_path = os.path.join(image_directory, image_file)\n",
        "        image_format = imghdr.what(image_path)\n",
        "\n",
        "        if image_format:\n",
        "            pass\n",
        "        else:\n",
        "            counter.append(image_path)\n",
        "            print(f\"Could not determine format for image '{image_file}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking image '{image_file}': {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY2QowlNcxao"
      },
      "source": [
        "##model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXs9NK2hcEvO",
        "outputId": "981fc8d5-b345-4ce7-b116-def64f11d65d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(475.0327056491576, 549.0049554013875)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Take an indication of the average images size\n",
        "\n",
        "links = os.listdir(r\"C:\\Users\\ONE BY ONE\\Desktop\\Images\\petra-Treasury\")\n",
        "heights=[]\n",
        "widths=[]\n",
        "for img in links:\n",
        "    try:\n",
        "        image = cv2.imread(rf\"C:\\Users\\ONE BY ONE\\Desktop\\Images\\petra-Treasury\\{img}\")\n",
        "        height, width, channels = image.shape\n",
        "        heights.append(height)\n",
        "        widths.append(width)\n",
        "    except:\n",
        "        pass\n",
        "np.average(heights),np.average(widths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXTNN4vEtMCa"
      },
      "outputs": [],
      "source": [
        "class ImageClassifier:\n",
        "    def __init__(self, img_size, batch_size, num_classes, learning_rate):\n",
        "        self.IMG_SIZE = img_size\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        self.NUM_CLASSES = num_classes\n",
        "        self.LEARNING_RATE = learning_rate\n",
        "        self.pretrained_model = None\n",
        "        self.datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.1,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "\n",
        "        self.generator = self._create_data_generator('training')\n",
        "        self.validation_generator = self._create_data_generator('validation')\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "    def _create_data_generator(self, subset):\n",
        "        return self.datagen.flow_from_directory(\n",
        "            r'C:\\Users\\ONE BY ONE\\Desktop\\Image classification\\images- 9 sites',\n",
        "            target_size=self.IMG_SIZE,\n",
        "            batch_size=self.BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            subset=subset\n",
        "        )\n",
        "\n",
        "    def VGG16_build_model(self):\n",
        "        base_model = self._get_pretrained_model()\n",
        "\n",
        "        model = models.Sequential()\n",
        "        model.add(base_model)\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(600, activation='relu'))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(250, activation='relu'))\n",
        "        model.add(layers.Dense(self.NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "        base_model.trainable = False\n",
        "\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=self.LEARNING_RATE),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "    def InceptionV3_build_model(self):\n",
        "        base_model = self._get_pretrained_model()\n",
        "\n",
        "        model = models.Sequential()\n",
        "        model.add(base_model)\n",
        "        model.add(layers.GlobalAveragePooling2D())  # Change from Flatten() to GlobalAveragePooling2D()\n",
        "        model.add(layers.Dense(1200, activation='relu'))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(700, activation='relu'))\n",
        "        model.add(layers.Dense(250, activation='relu'))\n",
        "        model.add(layers.Dense(self.NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "        base_model.trainable = False\n",
        "\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=self.LEARNING_RATE),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def VGG19_build_model(self):\n",
        "        base_model = self._get_pretrained_model()\n",
        "\n",
        "        model = models.Sequential()\n",
        "        model.add(base_model)\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(1000, activation='relu'))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(600, activation='relu'))\n",
        "        model.add(layers.Dense(250, activation='relu', kernel_regularizer=regularizers.l2(0.2)))\n",
        "\n",
        "        model.add(layers.Dense(self.NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "        base_model.trainable = False\n",
        "\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=self.LEARNING_RATE),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "    def ResNet101_build_model(self):\n",
        "        base_model = self._get_pretrained_model()\n",
        "\n",
        "        model = models.Sequential()\n",
        "        model.add(base_model)\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(1400, activation='relu'))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(800, activation='relu'))\n",
        "        model.add(layers.Dense(300, activation='relu'))\n",
        "        model.add(layers.Dense(self.NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "        base_model.trainable = False\n",
        "\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=self.LEARNING_RATE),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "    def _get_pretrained_model(self):\n",
        "        if self.pretrained_model == 'InceptionV3':\n",
        "            return InceptionV3(weights='imagenet', include_top=False, input_shape=(self.IMG_SIZE[0], self.IMG_SIZE[1], 3))\n",
        "        elif self.pretrained_model == 'EfficientNetB7':\n",
        "            return EfficientNetB7(weights='imagenet', include_top=False, input_shape=(self.IMG_SIZE[0], self.IMG_SIZE[1], 3))\n",
        "        elif self.pretrained_model == 'ResNet101':\n",
        "            return ResNet101(weights='imagenet', include_top=False, input_shape=(self.IMG_SIZE[0], self.IMG_SIZE[1], 3))\n",
        "        elif self.pretrained_model == 'VGG16':\n",
        "            return VGG16(weights='imagenet', include_top=False, input_shape=(self.IMG_SIZE[0], self.IMG_SIZE[1], 3))\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported pretrained model\")\n",
        "\n",
        "    def train_model(self, epochs):\n",
        "        history = self.model.fit(\n",
        "            self.generator,\n",
        "            epochs=epochs,\n",
        "            validation_data=self.validation_generator\n",
        "        )\n",
        "\n",
        "        accuracy = self.model.evaluate(self.validation_generator)[1]\n",
        "        print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    def plot_learning_curve(self, history):\n",
        "        # Extracting training and validation loss\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "\n",
        "        # Extracting training and validation accuracy (if available)\n",
        "        if 'accuracy' in history.history:\n",
        "            train_accuracy = history.history['accuracy']\n",
        "            val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "        # Plotting training and validation loss\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_loss, label='Training Loss')\n",
        "        plt.plot(val_loss, label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plotting training and validation accuracy (if available)\n",
        "        if 'accuracy' in history.history:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(train_accuracy, label='Training Accuracy')\n",
        "            plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "            plt.title('Training and Validation Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd6bQgetMCb"
      },
      "source": [
        "###InceptionV3_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmDX-1THtMCb"
      },
      "outputs": [],
      "source": [
        "pretrained_model_type = \"InceptionV3\"\n",
        "\n",
        "img_classifier = ImageClassifier(\n",
        "    img_size=(475, 550),\n",
        "    batch_size=32,\n",
        "    num_classes=9,\n",
        "    learning_rate=0.01\n",
        ")\n",
        "img_classifier.pretrained_model = pretrained_model_type\n",
        "img_classifier.InceptionV3_build_model()\n",
        "\n",
        "epochs_to_train = 20\n",
        "trained_history = img_classifier.train_model(epochs_to_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8eSGUVftMCb"
      },
      "outputs": [],
      "source": [
        "img_classifier.plot_learning_curve(trained_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeeRj7mrtMCc"
      },
      "source": [
        "###VGG19_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEF7ba1ptMCc"
      },
      "outputs": [],
      "source": [
        "pretrained_model_type = \"VGG19\"\n",
        "\n",
        "img_classifier = ImageClassifier(\n",
        "    img_size=(200, 250),\n",
        "    batch_size=32,\n",
        "    num_classes=9,\n",
        "    learning_rate=0.001\n",
        ")\n",
        "img_classifier.pretrained_model = pretrained_model_type\n",
        "img_classifier.VGG19_build_model_build_model()\n",
        "\n",
        "epochs_to_train = 30\n",
        "trained_history = img_classifier.train_model(epochs_to_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26iu-VPjtMCc"
      },
      "outputs": [],
      "source": [
        "img_classifier.plot_learning_curve(trained_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-2balNCtMCc"
      },
      "source": [
        "###VGG16_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEU2c4lMcEvR",
        "outputId": "6bfcc079-d853-405a-bf1d-3702ef65425a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "196/196 [==============================] - 316s 2s/step - loss: 0.9971 - accuracy: 0.6750 - val_loss: 0.6513 - val_accuracy: 0.7615\n",
            "Epoch 2/20\n",
            "196/196 [==============================] - 320s 2s/step - loss: 0.5796 - accuracy: 0.8103 - val_loss: 0.6515 - val_accuracy: 0.7603\n",
            "Epoch 3/20\n",
            "196/196 [==============================] - 318s 2s/step - loss: 0.5271 - accuracy: 0.8271 - val_loss: 0.5934 - val_accuracy: 0.7801\n",
            "Epoch 4/20\n",
            "196/196 [==============================] - 319s 2s/step - loss: 0.4474 - accuracy: 0.8506 - val_loss: 0.6195 - val_accuracy: 0.7750\n",
            "Epoch 5/20\n",
            "196/196 [==============================] - 316s 2s/step - loss: 0.3682 - accuracy: 0.8737 - val_loss: 0.5523 - val_accuracy: 0.8135\n",
            "Epoch 6/20\n",
            "196/196 [==============================] - 318s 2s/step - loss: 0.3745 - accuracy: 0.8742 - val_loss: 0.6021 - val_accuracy: 0.7955\n",
            "Epoch 7/20\n",
            "196/196 [==============================] - 315s 2s/step - loss: 0.3461 - accuracy: 0.8788 - val_loss: 0.6395 - val_accuracy: 0.7974\n",
            "Epoch 8/20\n",
            "196/196 [==============================] - 314s 2s/step - loss: 0.3406 - accuracy: 0.8820 - val_loss: 0.6192 - val_accuracy: 0.7929\n",
            "Epoch 9/20\n",
            "196/196 [==============================] - 314s 2s/step - loss: 0.3038 - accuracy: 0.8953 - val_loss: 0.6353 - val_accuracy: 0.8115\n",
            "Epoch 10/20\n",
            "196/196 [==============================] - 315s 2s/step - loss: 0.2889 - accuracy: 0.9005 - val_loss: 0.5859 - val_accuracy: 0.8096\n",
            "Epoch 11/20\n",
            "196/196 [==============================] - 311s 2s/step - loss: 0.2512 - accuracy: 0.9148 - val_loss: 0.6088 - val_accuracy: 0.7942\n",
            "Epoch 12/20\n",
            "196/196 [==============================] - 1308s 7s/step - loss: 0.2464 - accuracy: 0.9141 - val_loss: 0.6149 - val_accuracy: 0.8205\n",
            "Epoch 13/20\n",
            "196/196 [==============================] - 1562s 8s/step - loss: 0.2416 - accuracy: 0.9154 - val_loss: 0.5277 - val_accuracy: 0.8231\n",
            "Epoch 14/20\n",
            "196/196 [==============================] - 1000s 5s/step - loss: 0.2643 - accuracy: 0.9069 - val_loss: 0.5969 - val_accuracy: 0.8058\n",
            "Epoch 15/20\n",
            "196/196 [==============================] - 311s 2s/step - loss: 0.2256 - accuracy: 0.9213 - val_loss: 0.6001 - val_accuracy: 0.8135\n",
            "Epoch 16/20\n",
            "196/196 [==============================] - 308s 2s/step - loss: 0.1919 - accuracy: 0.9320 - val_loss: 0.6003 - val_accuracy: 0.8314\n",
            "Epoch 17/20\n",
            "196/196 [==============================] - 313s 2s/step - loss: 0.1761 - accuracy: 0.9401 - val_loss: 0.4998 - val_accuracy: 0.8449\n",
            "Epoch 18/20\n",
            "196/196 [==============================] - 309s 2s/step - loss: 0.1945 - accuracy: 0.9320 - val_loss: 0.5556 - val_accuracy: 0.8321\n",
            "Epoch 19/20\n",
            "196/196 [==============================] - 311s 2s/step - loss: 0.1850 - accuracy: 0.9369 - val_loss: 0.5811 - val_accuracy: 0.8218\n",
            "Epoch 20/20\n",
            "196/196 [==============================] - 310s 2s/step - loss: 0.1636 - accuracy: 0.9440 - val_loss: 0.6064 - val_accuracy: 0.8115\n",
            "49/49 [==============================] - 59s 1s/step - loss: 0.6064 - accuracy: 0.8115\n",
            "Validation Accuracy: 81.15%\n"
          ]
        }
      ],
      "source": [
        "pretrained_model_type = \"VGG16\"\n",
        "\n",
        "img_classifier = ImageClassifier(\n",
        "    img_size=(200, 250),\n",
        "    batch_size=32,\n",
        "    num_classes=9,\n",
        "    learning_rate=0.001\n",
        ")\n",
        "img_classifier.pretrained_model = pretrained_model_type\n",
        "img_classifier.VGG16_build_model_build_model()\n",
        "\n",
        "epochs_to_train = 30\n",
        "trained_history = img_classifier.train_model(epochs_to_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXBOz7l5tMCd"
      },
      "outputs": [],
      "source": [
        "img_classifier.plot_learning_curve(trained_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An-zPOEEtMCd"
      },
      "source": [
        "###RESNET101_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Qby-oDtMCd"
      },
      "outputs": [],
      "source": [
        "pretrained_model_type = \"ResNet101\"\n",
        "\n",
        "img_classifier = ImageClassifier(\n",
        "    img_size=(475, 550),\n",
        "    batch_size=32,\n",
        "    num_classes=9,\n",
        "    learning_rate=0.001\n",
        ")\n",
        "img_classifier.pretrained_model = pretrained_model_type\n",
        "img_classifier.ResNet101_build_model_build_model_build_model()\n",
        "\n",
        "epochs_to_train = 30\n",
        "trained_history = img_classifier.train_model(epochs_to_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXeWzHCxtMCd"
      },
      "outputs": [],
      "source": [
        "img_classifier.plot_learning_curve(trained_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ8xoWAA0jby"
      },
      "outputs": [],
      "source": [
        "train_accuracy_1 = np.array(history.history['accuracy'].copy())\n",
        "val_accuracy_1=  np.array(history.history['val_accuracy'].copy())\n",
        "train_loss_1 =  np.array(history.history['loss'].copy())\n",
        "val_loss_1 =  np.array(history.history['val_loss'].copy())\n",
        "train_accuracy_1,val_accuracy_1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO5V5y2-c2in"
      },
      "source": [
        "##Classification Report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6LO5Xl_cEvS",
        "outputId": "b7d1a809-a98a-4170-80d0-6a6a14aa2e56"
      },
      "outputs": [],
      "source": [
        "model.save('C:/Users/Anas/ONE BY ONE/Desktop/Models/vgg16-30epochs.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UtJn19B0jbz",
        "outputId": "a38e5fc0-f841-4622-d191-e157156f7451"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Ajloun Castle': 0,\n",
              " 'Hadrians Arch': 1,\n",
              " 'Petra-siq': 2,\n",
              " 'Roman Ruins-Jerash': 3,\n",
              " 'Roman amphitheater': 4,\n",
              " 'The Cardo Maximus of Jerash': 5,\n",
              " 'Wadi Rum': 6,\n",
              " 'petra-Treasury': 7,\n",
              " 'umm qais': 8}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqTkZxVn0jbz",
        "outputId": "89efe679-d946-412e-b053-f12d53a004b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "for _ in range(32):\n",
        "    # Get the next batch from the generator\n",
        "    batch_data, batch_labels = validation_generator.next()\n",
        "\n",
        "    # Make predictions using your model\n",
        "    predictions = model.predict(batch_data)\n",
        "\n",
        "    # Convert one-hot encoded labels to class indices\n",
        "    true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjQPNZHk0jbz",
        "outputId": "0f47fdc9-f631-4dfc-9679-02002aaae62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             precision    recall  f1-score   support\n",
            "\n",
            "              Ajloun Castle       0.87      0.97      0.92        67\n",
            "              Hadrians Arch       0.87      0.84      0.86        77\n",
            "                  Petra-siq       0.99      0.84      0.91        96\n",
            "         Roman Ruins-Jerash       0.60      0.48      0.53       136\n",
            "         Roman amphitheater       0.57      0.85      0.68       120\n",
            "The Cardo Maximus of Jerash       0.91      0.80      0.85       139\n",
            "                   Wadi Rum       0.87      0.96      0.91       135\n",
            "             petra-Treasury       0.91      0.98      0.94       125\n",
            "                   umm qais       0.87      0.67      0.75       129\n",
            "\n",
            "                   accuracy                           0.81      1024\n",
            "                  macro avg       0.83      0.82      0.82      1024\n",
            "               weighted avg       0.82      0.81      0.81      1024\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# class_names = generator.class_indices.keys()\n",
        "print(classification_report(true_labels, predicted_labels, target_names=class_names))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-8NMCZm0jbz",
        "outputId": "9a053714-5ba5-46a9-a076-02121306e215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model converted and saved to: C:\\Users\\ONE BY ONE\\Desktop\\Models\\vgg16-20epochs.onnx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load the TensorFlow model from the h5 file\n",
        "model_path = r\"C:\\Users\\ONE BY ONE\\Desktop\\Models\\vgg19-30epochs.h5\"\n",
        "modelt = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Convert the TensorFlow model to ONNX format\n",
        "onnx_model, _ = convert.from_keras(modelt)\n",
        "\n",
        "# Save the ONNX model to a file\n",
        "onnx_path = r\"C:\\Users\\ONE BY ONE\\Desktop\\Models\\vgg16-20epochs.onnx\"\n",
        "with open(onnx_path, 'wb') as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "print(f'Model converted and saved to: {onnx_path}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fe4JjsZQcS_m",
        "0-U9usZN0ooJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "image_env",
      "language": "python",
      "name": "image_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
